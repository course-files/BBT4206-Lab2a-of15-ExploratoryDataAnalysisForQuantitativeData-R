# *****************************************************************************
# Lab 1: Loading Datasets ----
#
# Course Code: BBT4206
# Course Name: Business Intelligence II
# Semester Duration: 21st August 2023 to 28th November 2023
#
# Lecturer: Allan Omondi
# Contact: aomondi [at] strathmore.edu
#
# Note: The lecture contains both theory and practice. This file forms part of
#       the practice. It has required lab work submissions that are graded for
#       coursework marks.
#
# License: GNU GPL-3.0-or-later
# See LICENSE file for licensing information.
# *****************************************************************************

# STEP 1. Create a new Project in R Studio ====
# Store this file inside the project directory
 
# Loading Datasets ====
## STEP 2: Download a sample dataset from the UCI Machine Learning repository.====
# Create a folder called "data" and store the following 2 files inside the "data" 
# folder
# Link 1: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
# Link 2: https://cdn.scribbr.com/wp-content/uploads/2020/03/crop.data_.anova_.zip
# Extract the "crop.data.csv" file into the data folder

## STEP 3. Load the downloaded sample data into R ====
# Load the datasets
irisDataset <- read.csv("data/iris.data", header=FALSE, stringsAsFactors=TRUE)
# names(irisDataset) <- c("sepal length in cm", "sepal width in cm", "petal length in cm", "petal width in cm", "class")
library(readr)
crop_data <- read_csv(
  "data/crop.data.csv",
  col_types = cols(
    density = col_factor(levels = c("1",
                                    "2")),
    block = col_factor(levels = c("1",
                                  "2", "3", "4")),
    fertilizer = col_factor(levels = c("1",
                                       "2", "3")),
    yield = col_double()
  )
)
View(crop_data)

## STEP 4. Load sample datasets provided with a package ====
library(mlbench)
data("PimaIndiansDiabetes")
data("BostonHousing")

# Dimensions ====
## STEP 5. Preview the Loaded Datasets ====
# Dimensions refer to the number of observations (rows) and the number of attributes/variables/features (columns). Execute the following commands to display the dimensions of your dataset:

dim(BostonHousing)
dim(crop_data)
dim(irisDataset)
dim(PimaIndiansDiabetes)

# Data Types ====
## STEP 6. Identify the Data Types ====
# Knowing the data types will help you to identify the most appropriate visualization types and algorithms that can be applied. It can also help you to identify the need to convert from categorical data (factors) to integers or vice versa where necessary. Execute the following command to identify the data types:
sapply(BostonHousing, class)
sapply(crop_data, class)
sapply(irisDataset, class)
sapply(PimaIndiansDiabetes, class)

# Descriptive Statistics ====
# 
# You must first understand your data before you can use it to design prediction models and to make generalizable inferences. It is not until you take the time to truly understand your dataset that you can fully comprehend the context of the results you achieve. This understanding can be done using descriptive statistics such as:
#   1.	Measures of frequency
# (e.g., count, percent)
# 
# 2.	Measures of central tendency 
# (e.g., mean, median, mode)
# 
# Further reading: https://www.scribbr.com/statistics/central-tendency/ 
#   
#   3.	Measures of distribution/dispersion/spread/scatter/variability
# (e.g., range, quartiles, interquartile range, standard deviation, variance, kurtosis, skewness)
# 
# Further reading: https://www.scribbr.com/statistics/variability/ 
#   Further reading: https://digitaschools.com/descriptive-statistics-skewness-and-kurtosis/
#   Further reading: https://www.scribbr.com/statistics/skewness/
#   
#   4.	Measures of relationship
# (e.g., covariance, correlation, ANOVA)
# 
# Further reading: https://www.k2analytics.co.in/covariance-and-correlation/ 
#   Further reading: https://www.scribbr.com/statistics/one-way-anova/ 
#   Further reading: https://www.scribbr.com/statistics/two-way-anova/ 
#   
#   Understanding your data can lead to:
#   (i)	Data cleaning: Removing bad data or imputing missing data.
# (ii)	Data transformation: Reduce the skewness by applying the same function to all the observations.
# (iii)	Data modelling: You may notice properties of the data such as distributions or data types that suggest the use (or to not use) specific algorithms.
# 


# Measures of Frequency ====

## STEP 7. Identify the number of instances that belong to each class. ====
# It is more sensible to count categorical variables (factors) than numeric variables, e.g., counting the number of male and female participants instead of counting the frequency of each participant’s height.
BostonHousing_freq <- BostonHousing$chas
cbind(frequency=table(BostonHousing_freq), percentage=prop.table(table(BostonHousing_freq))*100)

crop_data_density_freq <- crop_data$density
cbind(frequency=table(crop_data_density_freq), percentage=prop.table(table(crop_data_density_freq))*100)

crop_data_block_freq <- crop_data$block
cbind(frequency=table(crop_data_block_freq), percentage=prop.table(table(crop_data_block_freq))*100)

crop_data_fertilizer_freq <- crop_data$fertilizer
cbind(frequency=table(crop_data_fertilizer_freq), percentage=prop.table(table(crop_data_fertilizer_freq))*100)

irisDataset_freq <- irisDataset$V5
cbind(frequency=table(irisDataset_freq), percentage=prop.table(table(irisDataset_freq))*100)

PimaIndiansDiabetes_freq <- PimaIndiansDiabetes$diabetes
cbind(frequency=table(PimaIndiansDiabetes_freq), percentage=prop.table(table(PimaIndiansDiabetes_freq))*100)


# Measures of Central Tendency ====

## STEP 8. Calculate the mode ====
#Unfortunately, R does not have an in-built function for calculating the mode. We, therefore, must manually create a function that can calculate the mode.
BostonHousing_chas_mode <- names(table(BostonHousing$chas))[which(table(BostonHousing$chas) == max(table(BostonHousing$chas)))]
print(BostonHousing_chas_mode)

crop_data_fertilizer_mode <- names(table(crop_data$fertilizer))[which(table(crop_data$fertilizer) == max(table(crop_data$fertilizer)))]
print(crop_data_fertilizer_mode)

irisDataset_mode <- names(table(irisDataset$V5))[which(table(irisDataset$V5) == max(table(irisDataset$V5)))]
print(irisDataset_mode)

PimaIndiansDiabetes_mode <- names(table(PimaIndiansDiabetes$diabetes))[which(table(PimaIndiansDiabetes$diabetes) == max(table(PimaIndiansDiabetes$diabetes)))]
print(PimaIndiansDiabetes_mode)


# Measures of Distribution/Dispersion/Spread/Scatter/Variability ====

## STEP 9. Measure the distribution of the data for each variable ====
summary(BostonHousing)
summary(crop_data)
summary(irisDataset)
summary(PimaIndiansDiabetes)

## STEP 10. Measure the standard deviation of each variable ====
# Measuring the variability in the dataset is important because the amount of variability determines how well you can generalize results from the sample dataset to a new observation in the population.

# Low variability is ideal because it means that you can better predict information about the population based on sample data. High variability means that the values are less consistent, so it is harder to make predictions.

# The format “dataset[rows,columns]” can be used to specify the exact rows and columns to be considered. “dataset[,columns]” implies all rows will be considered. Specifying “BostonHousing[,-4]” implies all the columns except column number 4. This can also be stated as “BostonHousing[,c(1,2,3,5,6,7,8,9,10,11,12,13,14)]”. This allows us to calculate the standard deviation of only columns that are numeric, thus leaving out the columns termed as “factors” (categorical) or those that have a string data type.

sapply(BostonHousing[,-4], sd)
sapply(BostonHousing[,c(1,2,3,5,6,7,8,9,10,11,12,13,14)], sd)

# The data type of "yield" should be double (not numeric) so that it can be calculated.
sapply(crop_data[,4], sd)
sapply(irisDataset[,1:4], sd)
sapply(PimaIndiansDiabetes[,1:8], sd)

## STEP 11. Measure the variance of each variable ====
sapply(BostonHousing[,-4], var)
sapply(crop_data[,4], var)
sapply(irisDataset[,1:4], var)
sapply(PimaIndiansDiabetes[,1:8], var)

## STEP 12. Measure the kurtosis of each variable ====
# The Kurtosis informs you of how often outliers occur in the results.

# There are different formulas for calculating the kurtosis. Specifying “type = 2” allows us to use the 2nd formula which is the same kurtosis formula used in SPSS and SAS. More details about any function can be obtained by searching R help knowledge base:
  
# In “type = 2” (used in SPSS and SAS):
# 1.	Kurtosis < 3 implies a low number of outliers
# 2.	Kurtosis = 3 implies a medium number of outliers
# 3.	Kurtosis > 3 implies a high number of outliers

library(e1071)
sapply(BostonHousing[,-4],  kurtosis, type = 2)
sapply(crop_data[,4],  kurtosis, type = 2)
sapply(irisDataset[,1:4],  kurtosis, type = 2)
sapply(PimaIndiansDiabetes[,1:8],  kurtosis, type = 2)

## STEP 13. Measure the skewness of each variable ====

# The skewness informs you of the asymmetry of the distribution of results. Similar to kurtosis, there are several ways of computing the skewness. Using “type = 2” can be interpreted as:

# 1.	Skewness between -0.4 and 0.4 (inclusive) implies that there is no skew in the distribution of results; the distribution of results is symmetrical; it is a normal distribution.
# 2.	Skewness above 0.4 implies a positive skew; a right-skewed distribution.
# 3.	Skewness below -0.4 implies a negative skew; a left-skewed distribution.	 

sapply(BostonHousing[,-4],  skewness, type = 2)
sapply(crop_data[,4],  skewness, type = 2)
sapply(irisDataset[,1:4],  skewness, type = 2)
sapply(PimaIndiansDiabetes[,1:8],  skewness, type = 2)

# Note, executing:

# skewness(BostonHousing$crim, type=2)

# computes the skewness for one variable called “crim” in the BostonHousing dataset. However, executing the following enables you to compute the skewness for all the variables in the “BostonHousing” dataset except variable number 4:
  
# sapply(BostonHousing[,-4],  skewness, type = 2)


# Measures of Relationship ====

## STEP 14. Measure the covariance between variables ====
# Note that the covariance and the correlation are computed for numeric values only, not categorical values.
BostonHousing_cov <- cov(BostonHousing[,-4])
View(BostonHousing_cov)

crop_data_cov <- cov(crop_data[,4])
View(crop_data_cov)

irisDataset_cov <- cov(irisDataset[,1:4])
View(irisDataset_cov)

PimaIndiansDiabetes_cov <- cov(PimaIndiansDiabetes[,1:8])
View(PimaIndiansDiabetes_cov)

## STEP 15. Measure the correlation between variables ====
BostonHousing_cor <- cor(BostonHousing[,-4])
View(BostonHousing_cor)

crop_data_cor <- cor(crop_data[,4])
View(crop_data_cor)

irisDataset_cor <- cor(irisDataset[,1:4])
View(irisDataset_cor)

PimaIndiansDiabetes_cor <- cor(PimaIndiansDiabetes[,1:8])
View(PimaIndiansDiabetes_cor)


## STEP 16. Perform ANOVA on the “crop_data” dataset ====
# ANOVA (Analysis of Variance) is a statistical test used to estimate how a quantitative dependent variable changes according to the levels of one or more categorical independent variables. The null hypothesis (H0) of the ANOVA is that “there is no difference in means”, and the alternative hypothesis (Ha) is that “the means are different from one another”.

# The “aov()” function in R is used to calculate the test statistic for ANOVA. The test statistic is in turn used to calculate the p-value of your results. A p-value is a number that describes how likely you are to have found a particular set of observations if the null hypothesis were true. The smaller the p-value, the more likely you are to reject the null-hypothesis.

# The “crop_data” sample dataset loaded in STEP 4 contains observations from an imaginary study of the effects of fertilizer type and planting density on crop yield. In other words:
  
# Dependent variable:	Crop yield
# Independent variables:	Fertilizer type, planting density, and block

# The features (attributes) are:
# 1.	density: planting density (1 = low density, 2 = high density)
# 2.	block: planting location in the field (blocks 1, 2, 3, or 4)
# 3.	fertilizer: fertilizer type (type 1, 2, or 3)
# 4.	final crop yield (in bushels per acre)

# One-Way ANOVA can be used to test the effect of the 3 types of fertilizer on crop yield whereas, Two-Way ANOVA can be used to test the effect of the 3 types of fertilizer and the 2 types of planting density on crop yield.
crop_data_one_way_ANOVA <- aov(yield ~ fertilizer, data = crop_data)
summary(crop_data_one_way_ANOVA)

# This shows the result of each variable and the residual. The residual refers to all the variation that is not explained by the independent variable. The list below is a description of each column in the result:
# 1.	Df column: Displays the degrees of freedom for the independent variable (the number of levels (categories) in the variable minus 1), and the degrees of freedom for the residuals (the total number of observations minus the number of variables being estimated + 1, i.e., (df(Residuals)=n-(k+1)).

# 2.	Sum Sq column: Displays the sum of squares (a.k.a. the total variation between the group means and the overall mean). It is better to have a lower Sum Sq value for residuals.

# 3.  Mean Sq column: The mean of the sum of squares, calculated by dividing the sum of squares by the degrees of freedom for each parameter.

# 4.	F value column: The test statistic from the F test. This is the mean square of each independent variable divided by the mean square of the residuals. The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.

# 5.	Pr(>F) column: The p-value of the F statistic. This shows how likely it is that the F value calculated from the test would have occurred if the null hypothesis of “no difference among group means” were true.

# The three asterisk symbols (***) implies that the p-value is less than 0.001. P<0.001 can be interpreted as “the type of fertilizer used has an impact on the final crop yield”.

# We can also have a situation where the final crop yield depends not only on the type of fertilizer used but also on the planting density. A two-way ANOVA can then be used to confirm this. Execute the following for a two-way ANOVA (two independent variables):

crop_data_additive_two_way_ANOVA <- aov(yield ~ fertilizer + density, data = crop_data)
summary(crop_data_additive_two_way_ANOVA)

# Specifying an asterisk (*) instead of a plus (+) between the two independent variables (fertilizer * density) implies that they have an interaction effect rather than an additive effect. For example, an interaction effect would be that the fertilizer uptake by plants is affected by how close the plants are planted (density). An additive effect would be that the fertilizer uptake by plants is NOT affected by how close the plants are planted (density). Execute the following to perform a two-way ANOVA with the assumption that fertilizer and density have an interaction effect:
crop_data_interactive_two_way_ANOVA <- aov(yield ~ fertilizer * density, data = crop_data)
summary(crop_data_interactive_two_way_ANOVA)

# This can be interpreted as follows:
# The additive two-way ANOVA shows that the crop yield is affected by both the fertilizer and the density (P<0.001 for both independent variables). The interactive two-way ANOVA also shows that the crop yield is affected by both the fertilizer and the density (P<0.001 for both independent variables). However, the “fertilizer:density” variable has a high p-value (p=0.532500) which implies that there is not much variation in the dependent variable (final crop yield) that can be explained by the interaction between the independent variables (fertilizer and density).

# An ANOVA can also be performed on the variable called “block”. The “block” variable specifies the planting location in the field (blocks 1, 2, 3, or 4). Different blocks can have different control measures enforced to reduce the influence of confounding variables, e.g., temperature, water, soil quality, and plant species. This is common practice in research to ensure that the change in the dependent variable is correlated with the independent variable and not other variables that are not part of the research. Execute the following to add the “block” variable:
crop_data_interactive_two_way_ANOVA_with_block <- aov(yield ~ fertilizer + density + block, data = crop_data)
summary(crop_data_interactive_two_way_ANOVA_with_block)

# This can be interpreted as follows:
# The additive two-way ANOVA shows that the crop yield is affected by both the fertilizer and the density (P<0.001 for both independent variables). However, the block variable has a high p-value (p=0.488329) which implies that there is not much variation in the dependent variable (final crop yield) that can be explained by the “block” variable (the different blocks used to plant the crops).

# Basic Visualization for Understanding the Dataset ====

# Note: Ensure that the "Plots" window on the bottom right of R Studio has enough space to display the chart.

# The fastest way to improve your understanding of the dataset is to visualize it. Visualization can help you to spot outliers and give you an idea of possible data transformations you can apply. The basic visualizations to understand your dataset can be univariate visualizations (helps you to understand a single attribute) or multivariate visualizations (helps you to understand the interaction between attributes). Packages used to create visualizations include:
# (i)	Graphics package: Used to quickly create basic plots of data. This is the most appropriate to quickly understand the dataset before conducting further analysis.
# (ii)	Lattice package: Used to create more visually appealing plots of data.
# (iii)	ggplot2 package: Used to create even more visually appealing plots of data that can then be used to present the analysis results to the intended users. Given its complexity, it is not necessary to use ggplot2 to have a basic understanding of the dataset prior to further analysis.

# Note that the goal at this point is to understand your data, not to create visually appealing plots that are publicly shared. The visually appealing plots will be created much later after the best prediction model has been chosen.

## Univariate Plots ====
### STEP 17. Create Histograms for Each Numeric Attribute ====
# Histograms help in determining whether an attribute has a Gaussian distribution. They can also be used to identify the presence of outliers.

# Execute the following code to create histograms for the “BostonHousing” dataset:
par(mfrow=c(1,3))
for(i in 1:3) {
  hist(BostonHousing[,i], main=names(BostonHousing)[i])
}
hist(BostonHousing[,5], main=names(BostonHousing)[5])
hist(BostonHousing[,6], main=names(BostonHousing)[6])
hist(BostonHousing[,7], main=names(BostonHousing)[7])
hist(BostonHousing[,8], main=names(BostonHousing)[8])
hist(BostonHousing[,9], main=names(BostonHousing)[9])
hist(BostonHousing[,10], main=names(BostonHousing)[10])
hist(BostonHousing[,11], main=names(BostonHousing)[11])
hist(BostonHousing[,12], main=names(BostonHousing)[12])
hist(BostonHousing[,13], main=names(BostonHousing)[13])
hist(BostonHousing[,14], main=names(BostonHousing)[14])

# Execute the following code to create one histogram for attribute 4 (the only numeric column was “final crop yield (in bushels per acre)”) in the “crop_data” dataset:
# The code below converts column number 4 into unlisted and numeric data first so that a histogram can be plotted. Further reading: https://www.programmingr.com/r-error-messages/x-must-be-numeric-error-in-r-histogram/ )
crop_data_yield <- as.numeric(unlist(crop_data[,4]))
hist(crop_data_yield, main = names(crop_data)[4])

# Execute the following code to create histograms for attribute 1 to 4 of the “irisDataset” dataset:
par(mfrow=c(1,4))
for(i in 1:4) {
  hist(irisDataset[,i], main=names(irisDataset)[i])
}

# Execute the following code to create histograms for the “PimaIndiansDiabetes” dataset:
  
par(mfrow=c(1,8))
for(i in 1:8) {
  hist(PimaIndiansDiabetes[,i], main=names(PimaIndiansDiabetes)[i])
}



### STEP 18. Create Box and Whisker Plots for Each Numeric Attribute ====
# Box and whisker plots are very useful in understanding the distribution of data. 
# Further reading: https://www.scribbr.com/statistics/interquartile-range/

# Execute the following code to create box and whisker plots for the “BostonHousing” dataset:
# This considers the first 3 attributes which are numeric. The fourth attribute in the dataset is of the type “factor”, i.e., categorical.

par(mfrow=c(1,3))
for(i in 1:3) {
  boxplot(BostonHousing[,i], main=names(BostonHousing)[i])
}

# This considers the 5th to the 14th attributes which are numeric. The fourth attribute in the dataset is of the type “factor”, i.e., categorical

boxplot(BostonHousing[,5], main=names(BostonHousing)[5])
boxplot(BostonHousing[,6], main=names(BostonHousing)[6])
boxplot(BostonHousing[,7], main=names(BostonHousing)[7])
boxplot(BostonHousing[,8], main=names(BostonHousing)[8])
boxplot(BostonHousing[,9], main=names(BostonHousing)[9])
boxplot(BostonHousing[,10], main=names(BostonHousing)[10])
boxplot(BostonHousing[,11], main=names(BostonHousing)[11])
boxplot(BostonHousing[,12], main=names(BostonHousing)[12])
boxplot(BostonHousing[,13], main=names(BostonHousing)[13])
boxplot(BostonHousing[,14], main=names(BostonHousing)[14])

boxplot(crop_data[,4], main=names(crop_data)[4])
# or (if the line above results in an error, execute the code below to convert column number 4 into unlisted and numeric data first. Further reading: https://www.programmingr.com/r-error-messages/x-must-be-numeric-error-in-r-histogram/ )
crop_data_yield <- as.numeric(unlist(crop_data[,4]))
boxplot(crop_data_yield, main = names(crop_data)[4])

# Execute the following code to create box and whisker plots for attribute 1 to 4 of the “irisDataset” dataset:
par(mfrow=c(1,4))
for(i in 1:4) {
  boxplot(irisDataset[,i], main=names(irisDataset)[i])
}

# Execute the following code to create histograms for the “PimaIndiansDiabetes” dataset:
par(mfrow=c(1,8))
for(i in 1:8) {
  boxplot(PimaIndiansDiabetes[,i], main=names(PimaIndiansDiabetes)[i])
}

### STEP 19. Create Bar Plots for Each Categorical Attribute ====
# Categorical attributes (factors) can also be visualized. This is done using a bar chart to give an idea of the proportion of instances that belong to each category.

# Execute the following to create a bar plot for the categorical attribute 4 (“chas” - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)) in the “BostonHousing” dataset:

barplot(table(BostonHousing[,4]), main=names(BostonHousing)[4])

# The features (attributes) in the “crop_data” dataset are:
# 1.	density: planting density (1 = low density, 2 = high density)
# 2.	block: planting location in the field (blocks 1, 2, 3, or 4)
# 3.	fertilizer: fertilizer type (type 1, 2, or 3)
# 4.	final crop yield (in bushels per acre)
# 
# Execute the following to create a bar plot for the categorical attributes 1 to 3 in the “crop_data” dataset:
  
par(mfrow=c(1,3))
for(i in 1:3) {
  barplot(table(crop_data[,i]), main=names(crop_data)[i])
}



# The attributes in the “irisDataset” dataset are:
# 1.	sepal length in cm
# 2.	sepal width in cm
# 3.	petal length in cm
# 4.	petal width in cm
# 5.	class:
##   a.	Iris-Setosa
##   b.	Iris-Versicolour
##   c.	Iris-Virginica
# 1-4 are the Predictor (Independent) Variables
# 5 is the Target (Dependent) Variable

# Execute the following to create a bar plot for the categorical attribute 5 (class) in the “irisDataset” dataset:

barplot(table(irisDataset[,5]), main=names(irisDataset)[5])

# Execute the following to create a bar plot for attribute number 9 (diabetes – “pos” - had diabetes and “neg” - did not have diabetes) in the “PimaIndiansDiabetes” dataset:

barplot(table(PimaIndiansDiabetes[,9]), main=names(PimaIndiansDiabetes)[9])

### STEP 20. Create a Missingness Map to Identify Missing Data ====
# Some machine learning algorithms cannot handle missing data. A missingness map (also known as a missing plot) can be used to get an idea of the amount missing data in the dataset. The x-axis of the missingness map shows the attributes of the dataset whereas the y-axis shows the instances in the dataset. Horizontal lines indicate missing data for an instance whereas vertical lines indicate missing data for an attribute. The missingness map requires the “Amelia” package.
# 
# Execute the following to create a map to identify the missing data in each dataset:
library(Amelia)
missmap(BostonHousing, col=c("red", "grey"), legend=TRUE)
missmap(crop_data, col=c("red", "grey"), legend=TRUE)
missmap(irisDataset, col=c("red", "grey"), legend=TRUE)
missmap(PimaIndiansDiabetes, col=c("red", "grey"), legend=TRUE)

# As shown in the results, the 4 datasets that were loaded in this lab have no missing data. We can load a 5th dataset called “Soybean” found in the “mlbench” package for an example of a dataset that has missing data. Execute the following for an example of a dataset that has missing data:
library(mlbench)
data(Soybean)
View(Soybean)
missmap(Soybean, col=c("red", "grey"), legend=TRUE)

## Multivariate Plots ====

### STEP 21. Create a Correlation Plot ====
# Correlation plots can be used to get an idea of which attributes change together. The function “corrplot” found in the package “corrplot” is required. The larger the dot in the correlation plot, the larger the correlation. Blue represents a positive correlation whereas red represents a negative correlation.
# 
# 
# Execute the following to create correlation plots for 3 of the datasets loaded in STEP 2 to STEP 4:
library(corrplot)
corrplot(cor(BostonHousing[,-4]), method="circle")
corrplot(cor(irisDataset[,1:4]), method="circle")
corrplot(cor(PimaIndiansDiabetes[,1:8]), method="circle")

# The reason why the “crop_data” dataset has no correlation plot is because it does not have at least 2 numeric attributes to compare.

# Alternatively, the 'ggcorrplot::ggcorrplot()' function can be used to plot a more visually appealing plot.
# install.packages("ggcorrplot")
library(ggcorrplot)

ggcorrplot(cor(BostonHousing[,-4]))
ggcorrplot(cor(irisDataset[,1:4]))
ggcorrplot(cor(PimaIndiansDiabetes[,1:8]))

### STEP 22. Create a Scatter Plot ====
pairs(BostonHousing)
pairs(block~., data=crop_data, col=crop_data$block)
pairs(density~., data=crop_data, col=crop_data$density)
pairs(fertilizer~., data=crop_data, col=crop_data$fertilizer)
pairs(V5~., data=irisDataset, col=irisDataset$V5)
pairs(diabetes~., data=PimaIndiansDiabetes, col=PimaIndiansDiabetes$diabetes)

# Alternatively, the ggcorrplot package can be used to make the plots more appealing:
library(ggcorrplot)
ggplot(PimaIndiansDiabetes, aes(x=age, y=pregnant, shape=diabetes, color=diabetes)) + 
  geom_point()+
  geom_smooth(method=lm)

### STEP 23. Create Multivariate Box and Whisker Plots by Class ====
# This applies to datasets where the target (dependent) variable is categorical. Remember the following slides from the “Business Intelligence I” unit:
# Execute the following code:
library(caret)
featurePlot(x=irisDataset[,1:4], y=irisDataset[,5], plot="box")
featurePlot(x=PimaIndiansDiabetes[,1:8], y=PimaIndiansDiabetes[,9], plot="box")
